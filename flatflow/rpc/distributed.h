// Copyright 2024 The FlatFlow Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#ifndef FLATFLOW_RPC_DISTRIBUTED_H_
#define FLATFLOW_RPC_DISTRIBUTED_H_

#include <grpcpp/grpcpp.h>

#include <csignal>
#include <future>
#include <ranges>
#include <tuple>
#include <utility>
#include <vector>

#include "absl/base/log_severity.h"
#include "absl/log/check.h"
#include "absl/log/globals.h"
#include "absl/log/initialize.h"
#include "absl/log/internal/globals.h"
#include "absl/log/log.h"
#include "absl/strings/str_format.h"
#include "flatbuffers/grpc.h"

#include "flatflow/rpc/controlplane.grpc.fb.h"
#include "flatflow/rpc/controlplane_generated.h"
#include "flatflow/rpc/empty_generated.h"
#include "flatflow/scheduler/internal/scatter.h"
#include "flatflow/scheduler/scheduler.h"

namespace flatflow {

// flatflow::InitRequestBodyAdaptor
//
// Adaptor to ease sync up with types generated by the FlatBuffers compiler.
struct InitRequestBodyAdaptor {
  using return_type = typename remove_cvptr_t<
      decltype(std::declval<InitRequestBody>().sizes())>::return_type;
};

// flatflow::DistributedControlPlane
//
// A `flatflow::DistributedControlPlane` is an intermediary for communication
// between the scheduler and the data plane. It is responsible for exchanging
// the original computation schedule from the data plane with the reordered
// computation schedule and invoking callbacks exposed by the scheduler.
//
// Its primitives are based on the syntax of message passing interface (MPI);
// the control plane always starts with `Init` and ends with `Finalize`.
// At the beginning of each training epoch, `Scatter` is called to reorder
// the computation schedule of the data plane.
class DistributedControlPlane : public ControlPlane::Service {
 public:
  using size_type = typename Scheduler::size_type;

  // Constructors and assignment operators
  //
  // There are only basic constructors and assignment operators to allow copy
  // elision. The actual initialization is handled through `Init`.
  DistributedControlPlane() {}

  DistributedControlPlane(const DistributedControlPlane &other) = default;

  DistributedControlPlane &operator=(const DistributedControlPlane &other) =
      default;

  DistributedControlPlane(DistributedControlPlane &&other) = default;

  DistributedControlPlane &operator=(DistributedControlPlane &&other) = default;

  ~DistributedControlPlane() override {
    // The signal should be first validated as the program may have been
    // terminated via an external signal such as keyboard interrupt without
    // calling `Finalize`.
    if (signal_.valid() && signal_.get() != 0) {
      LOG(ERROR) << "Failed to send signal to the program";
    }
  }

  // DistributedControlPlane::Init()
  //
  // Initializes the training environment.
  grpc::Status Init(
      grpc::ServerContext *context,
      grpc::ServerReader<flatbuffers::grpc::Message<InitRequest>> *reader,
      flatbuffers::grpc::Message<Empty> *response) override {
    CHECK_NE(context, nullptr);
    CHECK_NE(reader, nullptr);
    CHECK_NE(response, nullptr);

    auto sizes = std::vector<typename InitRequestBodyAdaptor::return_type>();

    auto request = flatbuffers::grpc::Message<InitRequest>();
    while (reader->Read(&request)) {
      auto args = request.GetRoot();
      CHECK_NE(args, nullptr);

      auto body = args->body();
      auto trailer = args->trailer();
      CHECK_NE(body == nullptr, trailer == nullptr);

      if (body != nullptr) {
        CHECK_NE(body->sizes(), nullptr);
        total_size_ = body->total_size();
        sizes.reserve(total_size_);
        sizes.insert(sizes.end(), body->sizes()->begin(), body->sizes()->end());
      } else {
        data_parallel_rank_ = trailer->data_parallel_rank();
        data_parallel_world_size_ = trailer->data_parallel_world_size();
        global_batch_size_ = trailer->global_batch_size();

        // clang-format off
        LOG(INFO) << absl::StrFormat("Init called from %s (rank %u)", context->peer(), data_parallel_rank_);
        // clang-format on

        scheduler_ = Scheduler(data_parallel_world_size_, global_batch_size_,
                               trailer->micro_batch_size(), sizes.begin(),
                               sizes.end(), trailer->graph());
        scheduler_.on_train_start();
      }
    }

    auto builder = flatbuffers::grpc::MessageBuilder();
    const auto empty = CreateEmpty(builder);
    builder.Finish(empty);
    *response = builder.ReleaseMessage<Empty>();

    return grpc::Status::OK;
  }

  // DistributedControlPlane::Scatter()
  //
  // Exchanges the given computation schedule with the reordered
  // computation schedule from the scheduler.
  grpc::Status Scatter(
      grpc::ServerContext *context,
      grpc::ServerReaderWriter<flatbuffers::grpc::Message<ScatterResponse>,
                               flatbuffers::grpc::Message<ScatterRequest>>
          *stream) override {
    CHECK_NE(context, nullptr);
    CHECK_NE(stream, nullptr);

    // clang-format off
    LOG(INFO) << absl::StrFormat("Scatter called from %s (rank %u)", context->peer(), data_parallel_rank_);
    // clang-format on

    // If there is no previous computation schedule, the callback should not be
    // called as it is the beginning of the first epoch.
    if (!indices_.empty()) {
      scheduler_.on_epoch_end(epoch_);
    }

    auto indices = std::vector<size_type>();
    indices.reserve(total_size_);

    auto request = flatbuffers::grpc::Message<ScatterRequest>();
    while (stream->Read(&request)) {
      auto args = request.GetRoot();
      CHECK_NE(args, nullptr);
      CHECK_NE(args->indices(), nullptr);

      epoch_ = args->epoch();
      indices.insert(indices.end(), args->indices()->begin(),
                     args->indices()->end());
    }

    scheduler_.on_epoch_start(epoch_);

    auto schedule = std::vector<size_type>(indices.size());
    scheduler_.Schedule(indices.begin(), indices.end(), schedule.begin());

    indices_.resize(schedule.size() / data_parallel_world_size_);
    internal::Scatter(schedule.begin(), schedule.end(), indices_.begin(),
                      data_parallel_world_size_, data_parallel_rank_,
                      global_batch_size_);

    static constexpr auto stride = static_cast<size_type>(1 << 18);

    auto builder = flatbuffers::grpc::MessageBuilder();
    for (size_type offset = 0; offset < indices_.size(); offset += stride) {
      const auto slice =
          indices_ | std::views::drop(offset) | std::views::take(stride);
      const auto chunk = std::vector<size_type>(slice.begin(), slice.end());
      const auto resp =
          CreateScatterResponse(builder, builder.CreateVector(chunk));
      builder.Finish(resp);
      stream->Write(builder.ReleaseMessage<ScatterResponse>());
      builder.Clear();
    }

    return grpc::Status::OK;
  }

  // DistributedControlPlane::Finalize()
  //
  // Terminates the training environment.
  grpc::Status Finalize(grpc::ServerContext *context,
                        const flatbuffers::grpc::Message<Empty> *request,
                        flatbuffers::grpc::Message<Empty> *response) override {
    std::ignore = request;
    CHECK_NE(context, nullptr);
    CHECK_NE(response, nullptr);

    // clang-format off
    LOG(INFO) << absl::StrFormat("Finalize called from %s (rank %u)", context->peer(), data_parallel_rank_);
    // clang-format on

    scheduler_.on_epoch_end(epoch_);
    scheduler_.on_train_end();

    // The launch policy should be `std::launch::async`; otherwise a deadlock
    // will occur.
    signal_ = std::async(std::launch::async, std::raise, SIGTERM);

    auto builder = flatbuffers::grpc::MessageBuilder();
    const auto empty = CreateEmpty(builder);
    builder.Finish(empty);
    *response = builder.ReleaseMessage<Empty>();

    return grpc::Status::OK;
  }

 protected:
  size_type data_parallel_rank_;
  size_type data_parallel_world_size_;
  size_type epoch_;
  size_type global_batch_size_;
  size_type total_size_;
  std::vector<size_type> indices_;
  std::future<int> signal_;
  Scheduler scheduler_;
};

// flatflow::run()
//
// Executes the control plane. This routine is invoked from the Python frontend
// via foreign function interface (FFI); that is, there is no direct entry point
// to the control plane and the actual initialization and termination are made
// through `Init` and `Finalize`, respectively.
int run() {
  if (!absl::log_internal::IsInitialized()) {
    absl::InitializeLog();
    absl::SetStderrThreshold(absl::LogSeverity::kInfo);
  }

  // The operating system selects an available ephemeral port and `port` gets
  // populated with the selected port number.
  auto builder = grpc::ServerBuilder();
  auto port = 0;
  builder.AddListeningPort("[::1]:0", grpc::InsecureServerCredentials(), &port);

  static auto service = DistributedControlPlane();
  builder.RegisterService(&service);

  static auto server = builder.BuildAndStart();
  CHECK_NE(server, nullptr);
  CHECK_NE(port, 0);

  auto handler = [](int signal) {
    std::ignore = signal;
    server->Shutdown();
  };
  if (std::signal(SIGTERM, handler) == SIG_ERR) {
    LOG(ERROR) << "Failed to change handling of SIGTERM";
  }

  LOG(INFO) << absl::StrFormat("Control plane started on [::1]:%d", port);

  return port;
}

}  // namespace flatflow

#endif  // FLATFLOW_RPC_DISTRIBUTED_H_
