// Copyright 2025 The FlatFlow Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#ifndef FLATFLOW_OPS_OPS_H_
#define FLATFLOW_OPS_OPS_H_

#include <omp.h>

#include <functional>
#include <utility>

#include "absl/container/flat_hash_map.h"
#include "absl/log/check.h"
#include "absl/log/log.h"
#include "absl/strings/str_format.h"
#include "flatbuffers/base.h"
#include "flatbuffers/vector.h"

#include "flatflow/ops/graph_generated.h"
#include "flatflow/ops/internal/polynomial.h"
#include "flatflow/ops/node_generated.h"
#include "flatflow/ops/operator_generated.h"
#include "flatflow/types.h"

namespace flatflow {

// flatflow::SymIntAdaptor
//
// Adaptor to ease sync up with types generated by the FlatBuffers compiler.
struct SymIntAdaptor {
  using return_type = typename remove_cvptr_t<
      decltype(std::declval<SymInt>().data())>::return_type;
};

// flatflow::make_polynomial()
//
// Creates a `polynomial` whose coefficients are initialized from the
// corresponding arguments.
template <typename... Args>
internal::polynomial<typename SymIntAdaptor::return_type> make_polynomial(
    Args... args) {
  return internal::polynomial<typename SymIntAdaptor::return_type>(args...);
}

// flatflow::symbolic_trace_impl<>()
//
// This is a base template for implementing a symbolic transformation for the
// corresponding operator; calling this means that the program is ill-formed
// and should fail to compile.
template <Operator>
internal::polynomial<typename SymIntAdaptor::return_type> symbolic_trace_impl(
    const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *,
    const TensorMetadata *) = delete;

// flatflow::symbolic_trace_impl<_SOFTMAX>()
//
// Implements a symbolic transformation for `_softmax`.
//
// func: _softmax(Tensor self, int dim, bool half_to_float) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::_SOFTMAX>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // _softmax applies the softmax function to `self`.
  //
  // NOTE: _softmax requires five FLOPs for each element.
  // Approximate FLOPs breakdown can be found at
  // https://github.com/tensorflow/tensorflow/blob/v2.18.0/tensorflow/python/profiler/internal/flops_registry.py.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(5);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<_TO_COPY>()
//
// Implements a symbolic transformation for `_to_copy`.
//
// func: _to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None,
//                Device? device=None, bool? pin_memory=None,
//                bool non_blocking=False,
//                MemoryFormat? memory_format=None) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::_TO_COPY>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // _to_copy copies a tensor, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<_UNSAFE_VIEW>()
//
// Implements a symbolic transformation for `_unsafe_view`.
//
// func: _unsafe_view(Tensor self, SymInt[] size) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::_UNSAFE_VIEW>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // _unsafe_view is a tensor view operation, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<ADD_TENSOR>()
//
// Implements a symbolic transformation for `add.Tensor`.
//
// func: add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::ADD_TENSOR>(
    const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // add.Tensor adds `other`, scaled by `alpha`, to `self`.
  //
  // NOTE: As in the case of mul.Tensor, we have found that add.Scalar is
  // replaced by add.Tensor with a single argument due to constant folding.
  // That is, if two arguments are given, one addition and one multiplication
  // are required for each element; but if there is only one argument, just one
  // addition occurs for each element. add.Tensor also supports broadcasting to
  // a common shape, necessitating the use of output shape.
  CHECK_NE(args, nullptr);
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(args->size());

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<ADDMM>()
//
// Implements a symbolic transformation for `addmm`.
//
// func: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1,
//             Scalar alpha=1) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::ADDMM>(
    const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  CHECK_NE(args, nullptr);
  CHECK_EQ(args->size(), static_cast<flatbuffers::uoffset_t>(3));

  CHECK_NE(args->Get(1), nullptr);
  auto shape1 = args->Get(1)->shape();
  CHECK_NE(shape1, nullptr);
  CHECK_EQ(shape1->size(), static_cast<flatbuffers::uoffset_t>(2));

  CHECK_NE(args->Get(2), nullptr);
  auto shape2 = args->Get(2)->shape();
  CHECK_NE(shape2, nullptr);
  CHECK_EQ(shape2->size(), static_cast<flatbuffers::uoffset_t>(2));

  // addmm performs a matrix multiplication of the matrices `mat1` and `mat2`.
  // The matrix `self` is added to the final result.
  // `alpha` and `beta` are scaling factors on matrix-vector product between
  // `mat1` and `mat2` and the added matrix `self` respectively.
  // If `mat1` is a (n x m) tensor and `mat2` is a (m x p) tensor, then it
  // produces a (n x p) tensor with n x m x p MACs, i.e., 2 x n x m x p FLOPs.
  // For scaling and addition, it requires additional 3 x n x p FLOPs,
  // so totally n x p x (2 x m + 3) FLOPs are required.
  auto n = shape1->Get(0);
  CHECK_NE(n, nullptr);
  CHECK_NE(n->data(), nullptr);
  auto m = shape1->Get(1);
  CHECK_NE(m, nullptr);
  CHECK_NE(m->data(), nullptr);

  CHECK_NE(shape2->Get(0), nullptr);
  CHECK_NE(shape2->Get(0)->data(), nullptr);
  CHECK_EQ(m->data()->Get(0), shape2->Get(0)->data()->Get(0));
  CHECK_EQ(m->data()->Get(1), shape2->Get(0)->data()->Get(1));

  auto p = shape2->Get(1);
  CHECK_NE(p, nullptr);
  CHECK_NE(p->data(), nullptr);

  const auto poly = make_polynomial(m->data()->Get(0), m->data()->Get(1)) << 1;

  return make_polynomial(n->data()->Get(0), n->data()->Get(1)) *
         make_polynomial(p->data()->Get(0), p->data()->Get(1)) * (poly + 3);
}

// flatflow::symbolic_trace_impl<ALIAS>()
//
// Implements a symbolic transformation for `alias`.
//
// func: alias(Tensor(a) self) -> Tensor(a)
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::ALIAS>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // alias is a tensor view operation, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<ARANGE>()
//
// Implements a symbolic transformation for `arange`.
//
// func: arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None,
//              Device? device=None, bool? pin_memory=None) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::ARANGE>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // arange returns a tensor, so it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<ARANGE_START>()
//
// Implements a symbolic transformation for `arange.start`.
//
// func: arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None,
//                    Layout? layout=None, Device? device=None,
//                    bool? pin_memory=None) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::ARANGE_START>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // arange.start returns a tensor, so it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<BMM>()
//
// Implements a symbolic transformation for `bmm`.
//
// func: bmm(Tensor self, Tensor mat2) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::BMM>(
    const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  CHECK_NE(args, nullptr);
  CHECK_EQ(args->size(), static_cast<flatbuffers::uoffset_t>(2));

  CHECK_NE(args->Get(0), nullptr);
  auto shape0 = args->Get(0)->shape();
  CHECK_NE(shape0, nullptr);
  CHECK_EQ(shape0->size(), static_cast<flatbuffers::uoffset_t>(3));

  CHECK_NE(args->Get(1), nullptr);
  auto shape1 = args->Get(1)->shape();
  CHECK_NE(shape1, nullptr);
  CHECK_EQ(shape1->size(), static_cast<flatbuffers::uoffset_t>(3));

  // bmm performs a batch matrix-matrix product of matrices `self` and `mat2`.
  // `self` and `mat2` must be 3-D tensors each containing the same number of
  // matrices. If `self` is a (b x n x m) tensor and `mat2` is a (b x m x p)
  // tensor, then it produces a (b x n x p) tensor with b x n x m x p MACs,
  // i.e., 2 x b x n x m x p FLOPs.
  auto b = shape0->Get(0);
  CHECK_NE(b, nullptr);
  CHECK_NE(b->data(), nullptr);

  // The first dimension of `self` and `mat2` must be symbolically identical.
  CHECK_NE(shape1->Get(0), nullptr);
  CHECK_NE(shape1->Get(0)->data(), nullptr);
  CHECK_EQ(b->data()->Get(0), shape1->Get(0)->data()->Get(0));
  CHECK_EQ(b->data()->Get(1), shape1->Get(0)->data()->Get(1));

  auto n = shape0->Get(1);
  CHECK_NE(n, nullptr);
  CHECK_NE(n->data(), nullptr);
  auto m = shape0->Get(2);
  CHECK_NE(m, nullptr);
  CHECK_NE(m->data(), nullptr);

  // The last dimension of `self` and the middle dimension of `mat2` must be
  // symbolically identical.
  CHECK_NE(shape1->Get(1), nullptr);
  CHECK_NE(shape1->Get(1)->data(), nullptr);
  CHECK_EQ(m->data()->Get(0), shape1->Get(1)->data()->Get(0));
  CHECK_EQ(m->data()->Get(1), shape1->Get(1)->data()->Get(1));

  auto p = shape1->Get(2);
  CHECK_NE(p, nullptr);
  CHECK_NE(p->data(), nullptr);

  // coef4 is actually zero, since at least one of b, n, m, p is a constant.
  const auto poly = make_polynomial(b->data()->Get(0), b->data()->Get(1)) *
                    make_polynomial(n->data()->Get(0), n->data()->Get(1)) *
                    make_polynomial(m->data()->Get(0), m->data()->Get(1)) *
                    make_polynomial(p->data()->Get(0), p->data()->Get(1));

  return poly << 1;
}

// flatflow::symbolic_trace_impl<CAT>()
//
// Implements a symbolic transformation for `cat`.
//
// func: cat(Tensor[] tensors, int dim=0) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::CAT>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // cat concatenates tensors in the given dimension, so it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<CLONE>()
//
// Implements a symbolic transformation for `clone`.
//
// func: clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::CLONE>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // clone copies a tensor, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<COS>()
//
// Implements a symbolic transformation for `cos`.
//
// func: cos(Tensor self) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::COS>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // cos returns a new tensor with the cosine of the elements of `self`.
  //
  // NOTE: Its absolute number of FLOPs is implementation-dependent, typically
  // tens of FLOPs for each element; as this operator appears only once in the
  // embedding layer such as rotary position embedding (RoPE), we approximate
  // this to zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<CUMSUM>()
//
// Implements a symbolic transformation for `cumsum`.
//
// func: cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::CUMSUM>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // cumsum returns the cumulative sum of elements of `self` in the dimension
  // `dim`.
  //
  // NOTE: Its absolute number of FLOPs depends on the value of `dim`, which
  // cannot be deduced from the tensor metadata since cumsum always returns a
  // tensor with the same shape as `self`; the exact number of FLOPs is obtained
  // by subtracting one from the corresponding dimension and then multiplying
  // the size of the remaining dimensions, but here we ignore that subtraction.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(1);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<EMBEDDING>()
//
// Implements a symbolic transformation for `embedding`.
//
// func: embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1,
//                 bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::EMBEDDING>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // embedding is a dictionary lookup, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<EXPAND>()
//
// Implements a symbolic transformation for `expand`.
//
// func: expand(Tensor(a) self, SymInt[] size, *,
//              bool implicit=False) -> Tensor(a)
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::EXPAND>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // expand is a tensor view operation, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<FULL>()
//
// Implements a symbolic transformation for `full`.
//
// func: full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None,
//            Layout? layout=None, Device? device=None,
//            bool? pin_memory=None) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::FULL>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // full creates a tensor, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<GELU>()
//
// Implements a symbolic transformation for `gelu`.
//
// func: gelu(Tensor self, *, str approximate='none') -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::GELU>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // gelu applies the gaussian error linear unit (GELU) function to `self` in
  // element-wise.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(14);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<GT_TENSOR>()
//
// Implements a symbolic transformation for `gt.Tensor`.
//
// func: gt.Tensor(Tensor self, Tensor other) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::GT_TENSOR>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // gt.Tensor computes element-wise logical connectives, so it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<LT_TENSOR>()
//
// Implements a symbolic transformation for `lt.Tensor`.
//
// func: lt.Tensor(Tensor self, Tensor other) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::LT_TENSOR>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // lt.Tensor computes element-wise logical connectives, so it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<MASKED_FILL_SCALAR>()
//
// Implements a symbolic transformation for `masked_fill.Scalar`.
//
// func: masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::MASKED_FILL_SCALAR>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // masked_fill.Scalar is an out-of-place version of masked_fill_.Scalar,
  // so it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<MEAN_DIM>()
//
// Implements a symbolic transformation for `mean.dim`.
//
// func: mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *,
//                ScalarType? dtype=None) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::MEAN_DIM>(
    const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // mean.dim returns the mean value of each row of `self` in the given
  // dimension `dim`.
  CHECK_NE(args, nullptr);
  CHECK_EQ(args->size(), static_cast<flatbuffers::uoffset_t>(1));

  CHECK_NE(args->Get(0), nullptr);
  auto shape = args->Get(0)->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(1);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<MM>()
//
// Implements a symbolic transformation for `mm`.
//
// func: mm(Tensor self, Tensor mat2) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::MM>(
    const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  CHECK_NE(args, nullptr);
  CHECK_EQ(args->size(), static_cast<flatbuffers::uoffset_t>(2));

  CHECK_NE(args->Get(0), nullptr);
  auto shape0 = args->Get(0)->shape();
  CHECK_NE(shape0, nullptr);
  CHECK_EQ(shape0->size(), static_cast<flatbuffers::uoffset_t>(2));

  CHECK_NE(args->Get(1), nullptr);
  auto shape1 = args->Get(1)->shape();
  CHECK_NE(shape1, nullptr);
  CHECK_EQ(shape1->size(), static_cast<flatbuffers::uoffset_t>(2));

  // mm performs a matrix multiplication of the matrices `self` and `mat2`.
  // If `self` is a (n x m) tensor and `mat2` is a (m x p) tensor, then it
  // produces a (n x p) tensor with n x m x p MACs, i.e., 2 x n x m x p FLOPs.
  auto n = shape0->Get(0);
  CHECK_NE(n, nullptr);
  CHECK_NE(n->data(), nullptr);
  auto m = shape0->Get(1);
  CHECK_NE(m, nullptr);
  CHECK_NE(m->data(), nullptr);

  // The last dimension of `self` and the first dimension of `mat2` must be
  // symbolically identical.
  CHECK_NE(shape1->Get(0), nullptr);
  CHECK_NE(shape1->Get(0)->data(), nullptr);
  CHECK_EQ(m->data()->Get(0), shape1->Get(0)->data()->Get(0));
  CHECK_EQ(m->data()->Get(1), shape1->Get(0)->data()->Get(1));

  auto p = shape1->Get(1);
  CHECK_NE(p, nullptr);
  CHECK_NE(p->data(), nullptr);

  const auto poly = make_polynomial(n->data()->Get(0), n->data()->Get(1)) *
                    make_polynomial(m->data()->Get(0), m->data()->Get(1)) *
                    make_polynomial(p->data()->Get(0), p->data()->Get(1));

  return poly << 1;
}

// flatflow::symbolic_trace_impl<MUL_SCALAR>()
//
// Implements a symbolic transformation for `mul.Scalar`.
//
// func: mul.Scalar(Tensor self, Scalar other) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::MUL_SCALAR>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // mul.Scalar multiplies `self` by `other` in element-wise.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(1);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<MUL_TENSOR>()
//
// Implements a symbolic transformation for `mul.Tensor`.
//
// func: mul.Tensor(Tensor self, Tensor other) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::MUL_TENSOR>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // mul.Tensor multiplies `self` by `other` in element-wise.
  //
  // NOTE: As shown above, mul.Tensor gets two tensor arguments but we have
  // found that there is a bug during symbolic tracing where mul.Scalar is
  // replaced by mul.Tensor with a single argument due to constant folding.
  // Moreover, mul.Tensor supports broadcasting to a common shape, making it
  // difficult to trace FLOPs with input shapes alone; mul.Tensor has the same
  // FLOPs as mul.Scalar, and we leverage the output shape that reflects the
  // broadcasting.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(1);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<NATIVE_LAYER_NORM>()
//
// Implements a symbolic transformation for `native_layer_norm`.
//
// func: native_layer_norm(Tensor input, SymInt[] normalized_shape,
//                         Tensor? weight, Tensor? bias,
//                         float eps) -> (Tensor, Tensor, Tensor)
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::NATIVE_LAYER_NORM>(
    const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  CHECK_NE(args, nullptr);
  CHECK_EQ(args->size(), static_cast<flatbuffers::uoffset_t>(3));

  // native_layer_norm applies layer normalization over a mini-batch of inputs
  // `input`. The mean and standard-deviation are calculated over the last `D`
  // dimensions, where `D` is the dimension of `normalized_shape`. `weight` and
  // `bias` are learnable affine transform parameters of `normalized_shape`.
  CHECK_NE(args->Get(1), nullptr);
  CHECK_NE(args->Get(1)->shape(), nullptr);
  const auto D = args->Get(1)->shape()->size();

  CHECK_NE(args->Get(2), nullptr);
  CHECK_NE(args->Get(2)->shape(), nullptr);
  CHECK_EQ(args->Get(2)->shape()->size(), D);

  CHECK_NE(args->Get(0), nullptr);
  auto shape = args->Get(0)->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(7);

  for (flatbuffers::uoffset_t dim = 0; dim < D; ++dim) {
    CHECK_NE(args->Get(1)->shape()->Get(dim), nullptr);
    CHECK_NE(args->Get(1)->shape()->Get(dim)->data(), nullptr);
    CHECK_NE(args->Get(2)->shape()->Get(dim), nullptr);
    CHECK_NE(args->Get(2)->shape()->Get(dim)->data(), nullptr);
    CHECK_EQ(args->Get(1)->shape()->Get(dim)->data()->Get(0),
             args->Get(2)->shape()->Get(dim)->data()->Get(0));
    CHECK_EQ(args->Get(1)->shape()->Get(dim)->data()->Get(1),
             args->Get(2)->shape()->Get(dim)->data()->Get(1));

    const auto index = shape->size() - 1 - dim;
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  poly += 4;

  for (flatbuffers::uoffset_t index = 0; index < shape->size() - D; ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<NEG>()
//
// Implements a symbolic transformation for `neg`.
//
// func: neg(Tensor self) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::NEG>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // neg returns a new tensor with the negative of the elements of `self`.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(1);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<ONES>()
//
// Implements a symbolic transformation for `ones`.
//
// func: ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None,
//            Device? device=None, bool? pin_memory=None) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::ONES>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // ones creates a tensor, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<ONES_LIKE>()
//
// Implements a symbolic transformation for `ones_like`.
//
// func: ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None,
//                 Device? device=None, bool? pin_memory=None,
//                 MemoryFormat? memory_format=None) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::ONES_LIKE>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // ones_like creates a tensor, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<PERMUTE>()
//
// Implements a symbolic transformation for `permute`.
//
// func: permute(Tensor(a) self, int[] dims) -> Tensor(a)
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::PERMUTE>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // permute is a tensor view operation, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<POW_TENSOR_SCALAR>()
//
// Implements a symbolic transformation for `pow.Tensor_Scalar`.
//
// func: pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::POW_TENSOR_SCALAR>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // pow.Tensor_Scalar takes the power of each element in `self`
  // with `exponent`.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(1);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<RELU>()
//
// Implements a symbolic transformation for `relu`.
//
// func: relu(Tensor self) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::RELU>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // relu applies the rectified linear unit (ReLU) function to `self` in
  // element-wise.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(1);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<RSQRT>()
//
// Implements a symbolic transformation for `rsqrt`.
//
// func: rsqrt(Tensor self) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::RSQRT>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // rsqrt computes the element-wise reciprocal square root of `self`.
  //
  // NOTE: rsqrt requires two FLOPs for each element.
  // Approximate FLOPs breakdown can be found at
  // https://github.com/tensorflow/tensorflow/blob/v2.18.0/tensorflow/python/profiler/internal/flops_registry.py.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(2);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<RSUB_SCALAR>()
//
// Implements a symbolic transformation for `rsub.Scalar`.
//
// func: rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::RSUB_SCALAR>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // rsub.Scalar subtracts `self`, scaled by `alpha`, from `other`.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(2);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<SCALAR_TENSOR>()
//
// Implements a symbolic transformation for `scalar_tensor`.
//
// func: scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None,
//                     Device? device=None, bool? pin_memory=None) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::SCALAR_TENSOR>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // scalar_tensor creates a tensor, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<SILU>()
//
// Implements a symbolic transformation for `silu`.
//
// func: silu(Tensor self) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::SILU>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // silu applies the sigmoid linear unit (SiLU) function to `self`
  // in element-wise.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(4);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<SIN>()
//
// Implements a symbolic transformation for `sin`.
//
// func: sin(Tensor self) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::SIN>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // sin returns a new tensor with the sine of the elements of `self`.
  //
  // NOTE: For the same reason as cos, we approximate this to zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<SLICE_TENSOR>()
//
// Implements a symbolic transformation for `slice.Tensor`.
//
// func: slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None,
//                    SymInt? end=None, SymInt step=1) -> Tensor(a)
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::SLICE_TENSOR>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // slice.Tensor is a tensor view operation, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<SPLIT_TENSOR>()
//
// Implements a symbolic transformation for `split.Tensor`.
//
// func: split.Tensor(Tensor(a -> *) self, SymInt split_size,
//                    int dim=0) -> Tensor(a)[]
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::SPLIT_TENSOR>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // split.Tensor splits a tensor into chunks, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<SUB_TENSOR>()
//
// Implements a symbolic transformation for `sub.Tensor`.
//
// func: sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::SUB_TENSOR>(
    const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // sub.Tensor subtracts `other`, scaled by `alpha`, from `self`.
  //
  // NOTE: As in the case of mul.Tensor, we have found that sub.Scalar is
  // replaced by sub.Tensor with a single argument due to constant folding.
  // That is, if two arguments are given, one subtraction and one multiplication
  // are required for each element; but if there is only one argument, just one
  // subtraction occurs for each element. sub.Tensor also supports broadcasting
  // to a common shape, necessitating the use of output shape.
  CHECK_NE(args, nullptr);
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(args->size());

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<T>()
//
// Implements a symbolic transformation for `t`.
//
// func: t(Tensor(a) self) -> Tensor(a)
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::T>([[maybe_unused]] const flatbuffers::Vector<
                                     flatbuffers::Offset<TensorMetadata>> *args,
                                 [[maybe_unused]] const TensorMetadata *meta) {
  // t is a dimension swap, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<TANH>()
//
// Implements a symbolic transformation for `tanh`.
//
// func: tanh(Tensor self) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::TANH>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    const TensorMetadata *meta) {
  // tanh applies the hyperbolic tangent (tanh) function to `self`
  // in element-wise.
  CHECK_NE(meta, nullptr);

  auto shape = meta->shape();
  CHECK_NE(shape, nullptr);

  auto poly = make_polynomial(6);

  for (flatbuffers::uoffset_t index = 0; index < shape->size(); ++index) {
    CHECK_NE(shape->Get(index), nullptr);
    CHECK_NE(shape->Get(index)->data(), nullptr);
    poly *= make_polynomial(shape->Get(index)->data()->Get(0),
                            shape->Get(index)->data()->Get(1));
  }

  return poly;
}

// flatflow::symbolic_trace_impl<TRANSPOSE_INT>()
//
// Implements a symbolic transformation for `transpose.int`.
//
// func: transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::TRANSPOSE_INT>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // transpose.int is a dimension swap, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<TRIL>()
//
// Implements a symbolic transformation for `tril`.
//
// func: tril(Tensor self, int diagonal=0) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::TRIL>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // tril is a masking operation, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<TRIU>()
//
// Implements a symbolic transformation for `triu`.
//
// func: triu(Tensor self, int diagonal=0) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::TRIU>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // triu is a masking operation, so technically it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<UNSQUEEZE>()
//
// Implements a symbolic transformation for `unsqueeze`.
//
// func: unsqueeze(Tensor(a) self, int dim) -> Tensor(a)
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::UNSQUEEZE>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // unsqueeze inserts a singleton dimension at the specified position,
  // so it has zero FLOPs.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<VIEW>()
//
// Implements a symbolic transformation for `view`.
//
// func: view(Tensor(a) self, SymInt[] size) -> Tensor(a)
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::VIEW>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // view is a tensor view operation, so technically it has zero FLOPs.
  // See https://pytorch.org/docs/stable/tensor_view.html.
  return make_polynomial();
}

// flatflow::symbolic_trace_impl<WHERE_SELF>()
//
// Implements a symbolic transformation for `where.self`.
//
// func: where.self(Tensor condition, Tensor self, Tensor other) -> Tensor
template <>
internal::polynomial<typename SymIntAdaptor::return_type>
symbolic_trace_impl<Operator::WHERE_SELF>(
    [[maybe_unused]] const flatbuffers::Vector<
        flatbuffers::Offset<TensorMetadata>> *args,
    [[maybe_unused]] const TensorMetadata *meta) {
  // where.self is an element-wise branching operation, so it has zero FLOPs.
  return make_polynomial();
}

// flatflow::OperatorRegistry
//
// A `flatflow::OperatorRegistry` holds the key information to identify
// operators and generate optimized computation plans. It has an operator table
// in a form of `absl::flat_hash_map<Operator, ...>`, where each value contains
// a mapping from a pair of the corresponding operator and symbolic shapes to a
// symbolic expression for absolute number of floating point operations (FLOPs),
// which we call symbolic transformation.
// These expressions are created by measuring the absolute number of FLOPs or
// multiply-accumulates (MACs) for FMA instructions required for each pair of
// operator and symbolic shapes, for a given size such as sequence length.
//
// NOTE: To register a new operator, please follow the instructions below:
//
// * First, declare a new operator as an enumerator value of `Operator` in the
//   FlatBuffers schema - i.e., `flatflow/ops/operator.fbs`. The enumerator
//   values are recommended to be sorted in order of their original operator
//   names (not the enumerator values themselves) for searching convenience.
// * Second, generate codes from the updated schema by running `make generate`
//   in the root directory of this source tree. This will create
//   `flatflow/ops/operator_generated.h`, etc.
// * Third, add a template specialization of `symbolic_trace_impl` for the
//   new operator; please refer to the implementations above for supported
//   operators. Note that omitting this step will call the base template of
//   `symbolic_trace_impl`, which can be caught at compile time.
// * Finally, register the new operator to the operator table by calling
//   `OperatorRegistry::registerOperator` in the constructor below.
//   For the Python frontend, add the ATen counterpart and enumerator value as
//   key and value respectively to the operator table in `flatflow/ops/ops.py`.
class OperatorRegistry {
 public:
  // Constructors and assignment operators
  //
  // The constructor below creates the operator table and registers
  // symbolic transformations for all supported operators to it.
  //
  // CAVEATS
  //
  // We provide only a handful of ATen operator set for now. The operator set
  // is under development and more operators will be added in the future. For
  // expanding the operator set, please refer to the note above.
  OperatorRegistry() {
    constexpr auto kOpsTableSpace =
        sizeof(EnumValuesOperator()) / sizeof(Operator);
    ops_table_.reserve(kOpsTableSpace);

    registerOperator(Operator::_SOFTMAX,
                     &symbolic_trace_impl<Operator::_SOFTMAX>);
    registerOperator(Operator::_TO_COPY,
                     &symbolic_trace_impl<Operator::_TO_COPY>);
    registerOperator(Operator::_UNSAFE_VIEW,
                     &symbolic_trace_impl<Operator::_UNSAFE_VIEW>);
    registerOperator(Operator::ADD_TENSOR,
                     &symbolic_trace_impl<Operator::ADD_TENSOR>);
    registerOperator(Operator::ADDMM, &symbolic_trace_impl<Operator::ADDMM>);
    registerOperator(Operator::ALIAS, &symbolic_trace_impl<Operator::ALIAS>);
    registerOperator(Operator::ARANGE, &symbolic_trace_impl<Operator::ARANGE>);
    registerOperator(Operator::ARANGE_START,
                     &symbolic_trace_impl<Operator::ARANGE_START>);
    registerOperator(Operator::BMM, &symbolic_trace_impl<Operator::BMM>);
    registerOperator(Operator::CAT, &symbolic_trace_impl<Operator::CAT>);
    registerOperator(Operator::CLONE, &symbolic_trace_impl<Operator::CLONE>);
    registerOperator(Operator::COS, &symbolic_trace_impl<Operator::COS>);
    registerOperator(Operator::CUMSUM, &symbolic_trace_impl<Operator::CUMSUM>);
    registerOperator(Operator::EMBEDDING,
                     &symbolic_trace_impl<Operator::EMBEDDING>);
    registerOperator(Operator::EXPAND, &symbolic_trace_impl<Operator::EXPAND>);
    registerOperator(Operator::FULL, &symbolic_trace_impl<Operator::FULL>);
    registerOperator(Operator::GELU, &symbolic_trace_impl<Operator::GELU>);
    registerOperator(Operator::GT_TENSOR,
                     &symbolic_trace_impl<Operator::GT_TENSOR>);
    registerOperator(Operator::LT_TENSOR,
                     &symbolic_trace_impl<Operator::LT_TENSOR>);
    registerOperator(Operator::MASKED_FILL_SCALAR,
                     &symbolic_trace_impl<Operator::MASKED_FILL_SCALAR>);
    registerOperator(Operator::MEAN_DIM,
                     &symbolic_trace_impl<Operator::MEAN_DIM>);
    registerOperator(Operator::MM, &symbolic_trace_impl<Operator::MM>);
    registerOperator(Operator::MUL_SCALAR,
                     &symbolic_trace_impl<Operator::MUL_SCALAR>);
    registerOperator(Operator::MUL_TENSOR,
                     &symbolic_trace_impl<Operator::MUL_TENSOR>);
    registerOperator(Operator::NATIVE_LAYER_NORM,
                     &symbolic_trace_impl<Operator::NATIVE_LAYER_NORM>);
    registerOperator(Operator::NEG, &symbolic_trace_impl<Operator::NEG>);
    registerOperator(Operator::ONES, &symbolic_trace_impl<Operator::ONES>);
    registerOperator(Operator::ONES_LIKE,
                     &symbolic_trace_impl<Operator::ONES_LIKE>);
    registerOperator(Operator::PERMUTE,
                     &symbolic_trace_impl<Operator::PERMUTE>);
    registerOperator(Operator::POW_TENSOR_SCALAR,
                     &symbolic_trace_impl<Operator::POW_TENSOR_SCALAR>);
    registerOperator(Operator::RELU, &symbolic_trace_impl<Operator::RELU>);
    registerOperator(Operator::RSQRT, &symbolic_trace_impl<Operator::RSQRT>);
    registerOperator(Operator::RSUB_SCALAR,
                     &symbolic_trace_impl<Operator::RSUB_SCALAR>);
    registerOperator(Operator::SCALAR_TENSOR,
                     &symbolic_trace_impl<Operator::SCALAR_TENSOR>);
    registerOperator(Operator::SILU, &symbolic_trace_impl<Operator::SILU>);
    registerOperator(Operator::SIN, &symbolic_trace_impl<Operator::SIN>);
    registerOperator(Operator::SLICE_TENSOR,
                     &symbolic_trace_impl<Operator::SLICE_TENSOR>);
    registerOperator(Operator::SPLIT_TENSOR,
                     &symbolic_trace_impl<Operator::SPLIT_TENSOR>);
    registerOperator(Operator::SUB_TENSOR,
                     &symbolic_trace_impl<Operator::SUB_TENSOR>);
    registerOperator(Operator::T, &symbolic_trace_impl<Operator::T>);
    registerOperator(Operator::TANH, &symbolic_trace_impl<Operator::TANH>);
    registerOperator(Operator::TRANSPOSE_INT,
                     &symbolic_trace_impl<Operator::TRANSPOSE_INT>);
    registerOperator(Operator::TRIL, &symbolic_trace_impl<Operator::TRIL>);
    registerOperator(Operator::TRIU, &symbolic_trace_impl<Operator::TRIU>);
    registerOperator(Operator::UNSQUEEZE,
                     &symbolic_trace_impl<Operator::UNSQUEEZE>);
    registerOperator(Operator::VIEW, &symbolic_trace_impl<Operator::VIEW>);
    registerOperator(Operator::WHERE_SELF,
                     &symbolic_trace_impl<Operator::WHERE_SELF>);
  }

  OperatorRegistry(const OperatorRegistry &other) = default;

  OperatorRegistry &operator=(const OperatorRegistry &other) = default;

  OperatorRegistry(OperatorRegistry &&other) = default;

  OperatorRegistry &operator=(OperatorRegistry &&other) = default;

  // OperatorRegistry::registerOperator()
  //
  // Registers `target` to the operator table.
  void registerOperator(
      Operator target,
      internal::polynomial<typename SymIntAdaptor::return_type> (*func)(
          const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *,
          const TensorMetadata *)) {
    // TODO: Check if the insertion took place.
    ops_table_.try_emplace(target, func);
  }

  // OperatorRegistry::deregisterOperator()
  //
  // Removes `target` from the operator table.
  void deregisterOperator(Operator target) {
    // TODO: Check the number of elements removed.
    ops_table_.erase(target);
  }

  // OperatorRegistry::dispatch()
  //
  // Executes the symbolic transformation corresponding to the given operator.
  internal::polynomial<typename SymIntAdaptor::return_type> dispatch(
      Operator target,
      const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *args,
      const TensorMetadata *meta) const {
    CHECK(ops_table_.contains(target));
    return ops_table_.at(target)(args, meta);
  }

 protected:
  absl::flat_hash_map<
      Operator,
      internal::polynomial<typename SymIntAdaptor::return_type> (*)(
          const flatbuffers::Vector<flatbuffers::Offset<TensorMetadata>> *,
          const TensorMetadata *)>
      ops_table_;
};

// flatflow::symbolic_trace()
//
// Generates a perfect forwarding call wrapper for a function that evaluates
// FLOPs of the graph for a given size upon forward call.
decltype(auto) symbolic_trace(const Graph *graph) {
  CHECK_NE(graph, nullptr);

  auto nodes = graph->nodes();
  CHECK_NE(nodes, nullptr);

  const auto now = omp_get_wtime();

  const auto registry = OperatorRegistry();

  auto poly = make_polynomial();

  // clang-format off
  #pragma omp declare reduction(+ : internal::polynomial<           \
          typename SymIntAdaptor::return_type> : omp_out += omp_in) \
      initializer(omp_priv = omp_orig)

  #pragma omp parallel for reduction(+ : poly)
  for (flatbuffers::uoffset_t index = 0; index < nodes->size(); ++index) {
    auto node = nodes->Get(index);
    CHECK_NE(node, nullptr);
    poly += registry.dispatch(node->target(), node->args(), node->meta());
  }

  LOG(INFO) << absl::StrFormat("Traversing a graph with %u nodes took %fs", nodes->size(), omp_get_wtime() - now);
  // clang-format on

  // Here we ignore the constant term as it has no effect on differencing.
  poly[0] = 0;
  poly.normalize();

  return std::bind_front(
      internal::evaluate_polynomial<typename SymIntAdaptor::return_type,
                                    typename SymIntAdaptor::return_type>,
      poly);
}

}  // namespace flatflow

#endif  // FLATFLOW_OPS_OPS_H_
